{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire_detection_training.py\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ===========================\n",
    "# 1. Kaggle Setup and Dataset Download\n",
    "# ===========================\n",
    "def setup_kaggle_and_download():\n",
    "    from google.colab import files\n",
    "\n",
    "    # Upload kaggle.json\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    # Create .kaggle directory and move kaggle.json there\n",
    "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "    os.rename('kaggle.json', '/root/.kaggle/kaggle.json')\n",
    "    os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
    "\n",
    "    # Install Kaggle package\n",
    "    !pip install -q kaggle\n",
    "\n",
    "    # Download dataset from Kaggle\n",
    "    !kaggle datasets download -d christofel04/fire-detection-dataset\n",
    "\n",
    "    # Extract dataset\n",
    "    zip_path = '/content/fire-detection-dataset.zip'\n",
    "    extract_to = '/content'\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    print(f\"Dataset extracted to {extract_to}\")\n",
    "    print(\"Extracted contents:\")\n",
    "    print(os.listdir(extract_to))\n",
    "\n",
    "# ===========================\n",
    "# 2. Data Generators Setup\n",
    "# ===========================\n",
    "def create_data_generators(train_dir, val_dir, test_dir, target_size=(224,224), batch_size=32):\n",
    "    # Training data augmentation + rescale\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    # Validation and test data only rescale\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False  # Important for evaluation\n",
    "    )\n",
    "\n",
    "    return train_generator, val_generator, test_generator\n",
    "\n",
    "# ===========================\n",
    "# 3. SqueezeNet Model Definition\n",
    "# ===========================\n",
    "def SqueezeNetEnhanced(input_shape=(224, 224, 3), num_classes=2):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Conv1\n",
    "    x = layers.Conv2D(96, (7, 7), strides=2, padding='valid', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=2)(x)\n",
    "\n",
    "    def fire_module(x, squeeze_filters, expand_filters):\n",
    "        squeeze = layers.Conv2D(squeeze_filters, (1, 1))(x)\n",
    "        squeeze = layers.BatchNormalization()(squeeze)\n",
    "        squeeze = layers.ReLU()(squeeze)\n",
    "        expand1x1 = layers.Conv2D(expand_filters * 2, (1, 1), activation='relu')(squeeze)\n",
    "        expand3x3 = layers.Conv2D(expand_filters * 2, (3, 3), padding='same', activation='relu')(squeeze)\n",
    "        return layers.concatenate([expand1x1, expand3x3])\n",
    "\n",
    "    x = fire_module(x, 16, 64)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = fire_module(x, 16, 64)\n",
    "    x = fire_module(x, 32, 128)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=2)(x)\n",
    "\n",
    "    x = fire_module(x, 32, 128)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = fire_module(x, 48, 192)\n",
    "    x = fire_module(x, 48, 192)\n",
    "    x = fire_module(x, 64, 256)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=2)(x)\n",
    "\n",
    "    x = layers.Conv2D(num_classes, (1, 1))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Activation('softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ===========================\n",
    "# 4. Model Training and Evaluation for SqueezeNet\n",
    "# ===========================\n",
    "def train_and_evaluate_squeezenet(train_generator, val_generator, test_generator, epochs=10):\n",
    "    model = SqueezeNetEnhanced(input_shape=(224, 224, 3), num_classes=2)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator\n",
    "    )\n",
    "\n",
    "    model.save_weights(\"squeezenet_fire_weights.weights.h5\")\n",
    "\n",
    "    # Evaluate on test data\n",
    "    loss, accuracy = model.evaluate(test_generator)\n",
    "    print(f\"SqueezeNet Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Predictions for metrics\n",
    "    y_true = test_generator.classes\n",
    "    y_pred = model.predict(test_generator)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred_classes, target_names=list(test_generator.class_indices.keys())))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=test_generator.class_indices.keys(),\n",
    "                yticklabels=test_generator.class_indices.keys())\n",
    "    plt.title('Confusion Matrix - SqueezeNet')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# ===========================\n",
    "# 5. Focal Loss Definition for Class Imbalance\n",
    "# ===========================\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        weight = alpha * K.pow(1 - y_pred, gamma)\n",
    "        loss = weight * cross_entropy\n",
    "        return K.mean(K.sum(loss, axis=-1))\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# ===========================\n",
    "# 6. EfficientNetB3 Transfer Learning Model Creation\n",
    "# ===========================\n",
    "def create_efficientnet_model(input_shape=(224, 224, 3), num_classes=2):\n",
    "    base_model = EfficientNetB3(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ===========================\n",
    "# 7. Transfer Learning Training with Focal Loss and Fine-tuning\n",
    "# ===========================\n",
    "def train_efficientnet_with_focal_loss(train_dir, val_dir, test_dir, epochs=50, batch_size=32):\n",
    "    # Data generators with validation split (but since we use separate test_dir, val_dir should be validation set)\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        rescale=1./255\n",
    "    )\n",
    "\n",
    "    train_gen = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "labels = train_gen.classes\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "model = create_efficientnet_model(input_shape=(224, 224, 3), num_classes=2)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=focal_loss(alpha=0.25, gamma=2),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=epochs,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    ")\n",
    "\n",
    "# Unfreeze top layers for fine-tuning\n",
    "base_model = model.layers[0]\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze lower layers, unfreeze upper layers (e.g., last 30 layers)\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss=focal_loss(alpha=0.25, gamma=2),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_finetune = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=epochs // 2,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    ")\n",
    "\n",
    "model.save_weights(\"efficientnetb3_focal_finetuned.weights.h5\")\n",
    "\n",
    "# Evaluate\n",
    "loss, accuracy = model.evaluate(test_gen)\n",
    "print(f\"EfficientNetB3 Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_true = test_gen.classes\n",
    "y_pred = model.predict(test_gen)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "precision = precision_score(y_true, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_true, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred_classes, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=list(test_gen.class_indices.keys())))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=test_gen.class_indices.keys(),\n",
    "            yticklabels=test_gen.class_indices.keys())\n",
    "plt.title('Confusion Matrix - EfficientNetB3 Fine-tuned')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your dataset directories here after extracting\n",
    "train_dir = \"/content/fire-detection-dataset/train\"\n",
    "val_dir = \"/content/fire-detection-dataset/val\"\n",
    "test_dir = \"/content/fire-detection-dataset/test\"\n",
    "\n",
    "# Create data generators\n",
    "train_gen, val_gen, test_gen = create_data_generators(train_dir, val_dir, test_dir)\n",
    "\n",
    "# Train and evaluate SqueezeNet\n",
    "train_and_evaluate_squeezenet(train_gen, val_gen, test_gen, epochs=10)\n",
    "\n",
    "# Train and evaluate EfficientNetB3 with focal loss and fine tuning\n",
    "train_efficientnet_with_focal_loss(train_dir, val_dir, test_dir, epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7b771",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "- Replace paths `train_dir`, `val_dir`, and `test_dir` with your actual directories.\n",
    "- The Kaggle setup function is only needed if you want to automate dataset download inside Colab.\n",
    "- Training epochs are set to modest defaults, you can increase as needed.\n",
    "- Save weights only; you can save the full model with `model.save()` if preferred.\n",
    "- Evaluation prints detailed metrics and plots confusion matrix.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
